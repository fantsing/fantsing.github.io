<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Lee Sivan&#39;s Blog</title>
        <link>http://localhost:1313/</link>
        <description>Recent content on Lee Sivan&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <copyright>Lee Sivan</copyright>
        <lastBuildDate>Wed, 15 Oct 2025 13:52:23 +0800</lastBuildDate><atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Transformer</title>
        <link>http://localhost:1313/p/transformer/</link>
        <pubDate>Wed, 15 Oct 2025 13:52:23 +0800</pubDate>
        
        <guid>http://localhost:1313/p/transformer/</guid>
        <description>&lt;p&gt;Transformer is a typical Seq2Seq model.&lt;/p&gt;
&lt;p&gt;Attention mechanism: Selectively focus on the important parts and ignore the unimportant ones.&lt;/p&gt;
&lt;p&gt;Embedding: The process of mapping textual information into numerical information.&lt;/p&gt;
&lt;p&gt;Queries (query vectors), Keys (key vectors), Values (value vectors)&lt;/p&gt;
&lt;p&gt;Step 1 is to create three new vectors q, k, and v for each input vector of the encoder.&lt;/p&gt;
&lt;p&gt;Step 2 is to calculate a correlation score (Score)
&lt;/p&gt;
$$
Score_{1.1}=\mathbf{q}_1 \cdot \mathbf{k}_1=|q_1||k_1|\cos\theta\\
Score_{1.2}=\mathbf{q}_1 \cdot \mathbf{k}_2=|q_1||k_2|\cos\theta\\
...
$$&lt;p&gt;
Step 3 is to divide the correlation score (Score) by 8, which will make the gradient during the model training more stable.&lt;/p&gt;
&lt;p&gt;Step 4 involves obtaining the weight factors through Softmax.&lt;/p&gt;
&lt;p&gt;Step 5 is to multiply each value vector by the corresponding Softmax score.
&lt;/p&gt;
$$
\text{Attention}(Q, K, V) = \text{Softmax}\left( \frac{QK^{\mathrm{T}}}{\sqrt{d_k}} \right) V
$$&lt;p&gt;
Multi-heads self-attention mechanism&lt;/p&gt;
&lt;p&gt;Encoder-decoder, initially developed to solve the problem of text translation&lt;/p&gt;
&lt;p&gt;Cross-attention mechanism&lt;/p&gt;
</description>
        </item>
        <item>
        <title>About</title>
        <link>http://localhost:1313/about/</link>
        <pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/about/</guid>
        <description>&lt;p&gt;This is my personal blog website. I will post my study notes on it, and also share some project technical insights. On the one hand, it records the content one has learned so that one can quickly look it up when memory fails. On the other hand, it is also a way to exercise one&amp;rsquo;s writing skills. Thank you for your reading. If you have any related questions, please feel free to contact me via email: &lt;a class=&#34;link&#34; href=&#34;mailto:15542277303@163.com&#34; &gt;15542277303@163.com&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Archives</title>
        <link>http://localhost:1313/archives/</link>
        <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/archives/</guid>
        <description></description>
        </item>
        <item>
        <title>Search</title>
        <link>http://localhost:1313/search/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/search/</guid>
        <description></description>
        </item>
        
    </channel>
</rss>
